\documentclass[12pt]{article}

%\documentclass{amsart}
%\documentclass{scrartcl}
%\usepackage{changepage}
%\usepackage{scrextend}

\usepackage{amssymb,amsmath,amsthm}
% amssymb has empty set symbo
\usepackage{scrextend} % for \begin{addmargin}[0.55cm]{0cm} text \end{margin}

\usepackage{mathrsfs} % for \mathscr{P}
\usepackage{float}
\usepackage{enumitem}
\usepackage{hanging}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{pst-node}%

\newcommand{\n}{ \noindent }
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\U}{\mathcal{U}}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newcommand{\pwset}{\mathcal{P}}
%\newcommand{\pwset}{\mathscr{P}}
\DeclareMathOperator{\LO}{\mathcal{L}}

\DeclareMathOperator{\proj}{proj}
\newcommand{\vct}{\mathbf}
\newcommand{\vctproj}[2][]{\proj_{\vct{#1}}\vct{#2}}


%https://tex.stackexchange.com/questions/22252/how-to-typeset-function-restrictions
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

% from https://tex.stackexchange.com/questions/644238/drawing-the-phase-portrait-of-two-differential-equations
% \usepackage{pgfplots}
% \pgfplotsset{compat=1.8}
% \usepackage{amsmath}
% \usepackage{derivative}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usetikzlibrary{decorations.markings}
\usepackage{amsmath}
\usepackage{derivative}


%\DeclareFontFamily{U}{MnSymbolC}{}
%\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
%\DeclareFontShape{U}{MnSymbolC}{m}{n}{
%  <-6>    MnSymbolC5
%  <6-7>   MnSymbolC6
%  <7-8>   MnSymbolC7
%  <8-9>   MnSymbolC8
%  <9-10>  MnSymbolC9
%  <10-12> MnSymbolC10
%  <12->   MnSymbolC12%
%}{}
%\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\overtilde}[1]{\mkern 1.5mu\widetilde{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[left=1.2in, right=1.2in, top=1in, bottom=1in]{geometry}

\newtheorem*{proposition}{Proposition}
\newtheorem*{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem{conj}{Conjecture}[section]
\newtheorem*{example}{Example}
\newtheorem{theorem}{Theorem}[section]  % This enables \begin{theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem*{result}{Result}

\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\DeclareMathOperator{\Tr}{Tr} 

\makeatletter
\newcommand{\@giventhatstar}[2]{\left(#1\;\middle|\;#2\right)}
\newcommand{\@giventhatnostar}[3][]{#1(#2\;#1|\;#3#1)}
\newcommand{\giventhat}{\@ifstar\@giventhatstar\@giventhatnostar}
\makeatother


\setcounter{MaxMatrixCols}{20}

\title{STAT 653 - Notes \\
Introduction to Mathematical Statistics}

\setcounter{tocdepth}{1}
%-1 part
%0 chapter     
%1 section       
%2 subsection  
%3 subsubsection
%4 paragraph
%5 subparagraph

\begin{document}
\maketitle
\tableofcontents

\section{Statistical Model}

\begin{example}
A coin is tossed $n$ times. The data available is $X = (X_1, X_2, \ldots, X_n)$, 
where $X_i \in \{0, 1\}$. The assumptions are:

\begin{enumerate}
\item outcomes are independent.
\item $P(X_i = 1) = \theta \in \Theta$ where $\theta$ is an unknown parameter
and $\Theta$ is the parameter space. In this case $\Theta = [0,1]$.
\end{enumerate}

We need to estimate $\theta$ based on the data $X = (X_1, X_2, \ldots, X_n)$,
where $X_i$ are random variables before the experiment is conducted. \\

So we need to find an estimator $T(X_1, X_2, \ldots, X_n)$ of $\theta \in \Theta$.\\

\underline{\textbf{Possible Estimators}}

\begin{enumerate}

\item $\displaystyle T_1 := T_1 (X_1, X_2, \ldots, X_n) = \overbar{X}_n = \frac{1}{n} \sum_{i = 1}^{n} X_i$

\begin{remark}
\begin{enumerate}[label = (\alph*)]
\item $\displaystyle \mathbb{E}(T_1) = \mathbb{E}(\overbar{X}_n) = \mathbb{E}(X_1) = \theta$
for all $\theta \in \Theta$ then $T_1$ is unbiased estimator of $\theta$.

\item $\lim_{n \rightarrow \infty} P(| \overbar{X}_n - \theta| > \epsilon ) = 0$ for all
$\epsilon > 0$.
\end{enumerate}
\end{remark}


\begin{definition}
In general, if $\displaystyle \lim_{n \rightarrow \infty} P(|T(X_1, \ldots, X_n) - \theta|  \epsilon) = 0$
for all $\epsilon > 0$ and for all $\theta \in \Theta$, then we call 
$T(X_1, \ldots, X_n)$ \textbf{consistent}.
\end{definition}

\item $T_2(X_1, \ldots, X_n) := X_1$, where $X_1 \in \{0, 1\}$. Then
$\mathbb{E}(T_2) = \mathbb{E}(X_1) = \theta$ for all $\theta \in \Theta$.\\

$T_2$ is unbiased but is not \underline{consistent}.

\item 
\begin{align*}
T_3 &:= T_3(X_1, \ldots, X_n) \\
&= \sqrt{\frac{1}{\lfloor \frac{n}{2} \rfloor} \sum_{i = 1}^{\lfloor \frac{n}{2} \rfloor} X_{2i} X_{2i-1} }
\end{align*}

$T_3$ is biased because

\begin{align*}
\mathbb{E}(T_3) &\leq \sqrt{\frac{1}{\lfloor \frac{n}{2} \rfloor} \sum_{i = 1}^{\lfloor \frac{n}{2} \rfloor} X_{2i} X_{2i-1} } \\
&= \theta \quad\quad \forall \theta \in \Theta
\end{align*}

\end{enumerate}
\end{example}

\begin{example}
Suppose $X_1, X_2, \ldots, X_n$ are independent and have uniform$[0, \theta]$,
where $theta \in \Theta = \mathbb{R}_+$. So $\Theta = \{\theta : \theta > 0\}$. \\


\underline{\textbf{Possible Estimators}}

\begin{enumerate}
\item $T_1(X_1, \ldots, X_n) = 2\overbar{X}_n$

\item $T_2(X_1, \ldots, X_n) = X_{(n)}$ (max)

\item $T_3(X_1, \ldots, X_n) = c_n X_{(n)}$

Correct the max by a constant so it is unbiased.
\end{enumerate}
\end{example}

\begin{example}
We want to receive a shipment of oranges and suspect that part of them
rot off. To check the shipment we draw a random sample without replacement
of size $n$ from the shipment (population) of size $N$.\\

Let $\theta$ be the proportion of bad oranges in the population. So
$\Theta = \{ \frac{0}{N}, \frac{1}{N}, \ldots, \frac{N}{N} \}$.\\

Let

\begin{equation*}
X_i =
\begin{cases*}
0 & if good \\
1 & if bad
\end{cases*}
\end{equation*}

for $i = 1, 2, \ldots, n$ and let $X = (X_1, X_2, \ldots, X_n)$.\\

Let $\displaystyle T_1(X) = \sum^{n}_{i = 1} X_i$. Then $T_1$ has a
hypergeometric distribution. So

\begin{equation*}
P_{\theta}(X_1 = k) = \frac{ \left( \frac{N\theta}{k} \right) \left( \frac{N - N\theta}{n-k} \right)}
{ \left( \frac{N}{n} \right) }
\end{equation*}

for $k \in \{ \max(0, n - (N - N\theta), \ldots, \min(n, N\theta))\}$
\end{example}

\section{The Likelihood Function}

$$ X \sim P_{\theta}, \quad\quad \theta \in \Theta $$

We have 2 cases for now (discrete and continuous):

\begin{enumerate}[label = (R\arabic*)]
\item $P_{\theta}$ is defined by a joint pdf $f_X (x; \theta)$ for all $\theta \in \Theta$.
\item $P_{\theta}$ is defined by a joint pmf $P(X = x; \theta)$ for all $\theta \in \Theta$.
\end{enumerate}

\begin{definition}
Let $P_{\theta}$, $\theta \in \Theta$ be a model satisfying (R1) or (R2). Then the function

\begin{equation*}
L(x; \theta) =
\begin{cases*}
f_X(x; \theta) & if (R1) \\
P(X = x; \theta) & if (R2)
\end{cases*}.
\end{equation*}
\end{definition}

\begin{example}
Not (R1) and not (R2).

Let

$$ X \sim N(\theta, 1) \quad\quad \theta \in \Theta = \mathbb{R}. $$

We observe $Y = \max(0, X)$, 

\begin{equation*}
Y =
\begin{cases*}
0 & if $X \leq 0 $\\
X & if $ X > 0$
\end{cases*}
= X I(X > 0)
\end{equation*}

where $I(\cdot)$ is the indicator function.\\

$F_{\theta} (t) = P(Y \leq t)$ for all $t \in \mathbb{R}$.
\end{example}

\begin{example}
Back to oranges example where $X = (X_1, X_2, \ldots, X_n)$ is the data
and $\Theta = \{ \frac{0}{N}, \frac{1}{N}, \ldots, \frac{N}{N} \}$. Let
$\displaystyle T(X) = \sum^{n}_{i=1} X_i$. Then

\begin{align*}
L(x; \theta) &= P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) \\
&= P_{\theta}\left(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n, T(X) = \sum^{n}_{i=1} x_i \right) \\
&= P_{\theta}\left(T(X) = \sum^{n}_{i=1} x_i\right)P\left(X_1 = x_1, \ldots, X_n = x_n \; \middle|\;  T(X) = \sum^{n}_{i=1} x_i \right).
\end{align*}


Now define $K_n = \sum^{n}_{i=1} x_i$. For example, if $n = 5$ and 
we observed $(1, 0, 0, 1, 1)$ then

$$ K = \sum_{i=1}^{5} x_i = 3. $$

Since there are 10 possibilities for which entries are 1 versus 0,
$\binom{5}{3} = 10$. Because all possible combinations of 1 and 0 
are possible we can use symmetry to calculate the probability of
any particular sequence of 1 and 0 as $1/\binom{5}{3}$. We use 
this reasoning below to derive the expression on the right. 

Then

\begin{equation*}
L(x; \theta) = \frac{ \binom{N\theta}{K_n} \binom{N-N\theta}{n - K_n}}
{\binom{N}{n}} \times \frac{1}{\binom{n}{K_n}}.
\end{equation*}
\end{example}

\section{Identifiability of Statistical Models}

\begin{definition}
Let $X \sim P_{\theta}$, $\theta \in \Theta$. A model $P_{\theta}$,
$\theta \in \Theta$ is \underline{identifiable} if for any pair 
$(\theta, \theta^{\prime})$ such that $\theta \neq \theta^{\prime}$
and $\theta, \theta^{\prime} \in \Theta$, then
$P_{\theta} \neq P_{\theta^{\prime}}$.
\end{definition}

\begin{remark}
This means that there is an event $A$, such that 
$P_{\theta}(A) \neq P_{\theta^{\prime}}$ where 
$\theta \neq \theta^{\prime}$.

\begin{enumerate}[label = R(\arabic*)]
\item For $\theta \neq \theta^{\prime}$, 
$f(x; \theta) \neq f(x; \theta^{\prime})$ for any neighborhood of
$x$ (an open ball $B(x, r)$ centered at $x$).

By open ball we mean $B(x, r) = \{y : |x-y| < \epsilon \}$ where 
$|v| = (\sum^{n}_{i = 1} v_i^2 )^{1/2}$ (euclidean norm).

\item Discrete support, for some $x$
$P_{\theta}(X = x) \neq P_{\theta^{\prime}}(X = x)$ where
$\theta \neq \theta^{\prime}$.

\end{enumerate}
\end{remark}

\begin{example}
Suppose we observe $X_1, X_2, \ldots, X_n$ where 
$X_i = \theta \cdot Z_i \sim N(0, \theta^2)$ and
$Z_i \sim N(0,1)$ and $\theta \in \Theta = \mathbb{R} \setminus \{0\}$.\\

If $\theta_1 = 1 \neq -1 = \theta_2$, then

\begin{equation*}
L(x_1, x_2, \ldots, x_n; \theta = 1) = L(x_1, x_2, \dots, x_n; \theta = -1)
\end{equation*}

for any $x = (x_1, \ldots, x_n)$.
\end{example}

\begin{result}
The model $\{P_{\theta}, \theta \in \Theta \}$ is identifiable if there exists 
a statistic $T(X)$ ($X \sim P_{\theta}, \theta \in \Theta$) where 
expectation is a one-to-one function of $\theta \in \Theta$, i.e., such that

\begin{equation}\label{eq1}
\forall (\theta, \theta^{\prime}), \quad \theta \neq \theta^{\prime} \implies 
\mathbb{E}_{\theta}(T(X)) \neq \mathbb{E}_{\theta^{\prime}}(T(X))
\end{equation}

\begin{proof}
We use proof by contradiction. Suppose that (\ref{eq1}) holds, but there
exists $\theta \neq \theta^{\prime}$ such that $P_{\theta} = P_{\theta^{\prime}}$.
If so, then $\mathbb{E}_{\theta}(T(X)) = \mathbb{E}_{\theta^{\prime}}(T(X))$,
which contradicts (\ref{eq1}).
\end{proof}
\end{result}


In the previous example, $\theta = 1$, $\theta^{\prime} = -1$.

\begin{example}
Let $X_1, X_2, \ldots, X_n \overset{\mathrm{iid}}{\sim} \text{Bernoulli}(\theta)$
where $\theta \in \Theta = [0, 1]$. We will show that $\theta$ is identifiable
using the definition and also the above result.\\

Let $\theta$ and $\theta^{\prime}$ be arbitrary and suppose 
$\theta \neq \theta^{\prime}$ and $\theta^{\prime}$,$\theta \in \Theta$. Also
suppose $X = (1, 1, \ldots, 1)$. Then

\begin{align*}
P_{\theta}(X_1, X_2, \ldots, X_n) &= \theta^{n}\\
P_{\theta^{\prime}}(X_1 = 1, \ldots, X_n) &= (\theta^{\prime})^{n}.
\end{align*}

Since $\theta \in [0,1]$ then $\theta^n \neq (\theta^{\prime})^n$ and the model
is identifiable.\\

Now take a statistic $T(X_1, \ldots, X_n) = X_1$ (or we could take 
$T(X_1, \ldots, X_n) = \sum^{n}_{i=1} X_i$ or
$T(X_1, \ldots, X_n) = \sum^{n}_{i=1} \overbar{X}_n$).\\

For any 
$(\theta, \theta^{\prime}) \in \Theta$, if $\theta \neq \theta^{\prime}$
then $\mathbb{E}_{\theta}(\overbar{X}_n) = \theta \neq \theta^{\prime} = 
\mathbb{E}_{\theta^{\prime}}(\overbar{X}_n)$. Then by the above result
the model is identifiable.
\end{example}

\begin{example}
\begin{equation*}
X_1, \ldots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)
\end{equation*}

\begin{enumerate}[label = Part \arabic*)]

\item 
Let $\theta = (\mu, \sigma^2) \in \Theta = \mathbb{R} \times \mathbb{R}^2$. Then

\begin{equation*}
L(x_1, \ldots, x_n; \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}} 
e^{\frac{-(x_i - \mu)^2}{2\sigma^2}} I(\mu \in \mathbb{R})I(\sigma^2 > 0).
\end{equation*}

It is difficult in this case to use the definition to show identifiability
in this case, but we can use the previous result.

We are given $X = (X_1, X_2, \dots, X_N)$. Let 

\begin{equation*}
T(X) = \left( \sum_{i = 1}^n X_i,  \sum^n_{i = 1} X_i^2 \right)
\end{equation*}

Then 
\begin{align*}
\mathbb{E}_{\theta}(T) &= (n \mu, n(\sigma^2 + \mu^2)),\\
\mathbb{E}_{\theta^{\prime}}(T) &= (n \mu^{\prime}, n(\sigma^{2^{\prime}} + (\mu^{\prime})^2) 
\end{align*}

Thus, if $(\theta, \theta^2) \in \Theta$ then
\begin{equation*}
\forall \theta \neq \theta^{\prime} \implies \mathbb{E}_{\theta}(T(X)) \neq \mathbb{E}_{\theta^{\prime}}(T(X)).
\end{equation*}

If $\theta \neq \theta^{\prime}$ then $\mu \neq \mu^{\prime}$ or 
$\sigma^2 \neq \sigma^{2^{\prime}}$ or $\mu \neq \mu^{\prime}$ and
$\sigma^2 \neq \sigma^{2^{\prime}}$. In all three cases then
$\mathbb{E}_{\prime}(T(X)) \neq \mathbb{E}_{\theta^{\prime}}(T(X))$.

\item Suppose we observe only $Y_1, \ldots, Y_n$ where

\begin{equation*}
Y_i =
\begin{cases}
+1 & \text{if } X_i \geq 0 \\
-1 & \text{if } X_i < 0.
\end{cases}
\end{equation*}

Since $Y_i = g(X_i)$ and the $X_i$'s are independent, then the $Y_i$'s
are also independent.

Then the likelihood function is

\begin{align*}
L(y_i, \ldots, y_n; \theta) &= \prod_{i = 1}^n P(Y_i = y_i; \theta) \\
&= \prod_{i = 1}^n \left[ I(y_i = 1) P(X_i \geq 0) + I(y_i = -1)P(X_i < 0) \right].
\end{align*}

Now note that

\begin{align*}
P(X_i \geq 0) &= 1 - P(X_i < 0) = 1 - \Phi \left( -\frac{\mu}{\sigma} \right) = \Phi \left( \frac{\mu}{\sigma} \right)\\
P(X_i < 0) &= \Phi \left( -\frac{\mu}{\sigma} \right)
\end{align*}

so that only the ratio $\mu/\sigma$ matters for the the likelihood.\\

Now let $\theta = (3, 9) \neq (4, 16) = \theta^{\prime}$. For $\theta$ we
have $\mu/\sigma = 3/3 = 1$ and for $\theta^{\prime}$ we have
$\mu/\sigma = 4/4 = 1$. Thus we have

$$ \theta = (3,9) \neq (4,16) = \theta^{\prime} \implies L(y; \theta) = L(y; \theta^{\prime}) $$

and so the model is not identifiable. For any $y = (y_1, \ldots, y_n)$ we
have $L(y; \theta) = L(y; \theta^{\prime})$ and thus the model is
not identifiable.


\begin{remark}
Above we used the fact that for a general normal random variable $N(\mu, \sigma^2)$,
$F(x) = \Phi((x - \mu)/\sigma)$.
\end{remark}
\end{enumerate}
\end{example}

\section{Sufficient Statistic}

\begin{definition}
Let $X \sim P_{\theta}$, $\theta \in \Theta$ and we observe data 
$X = (X_1, \ldots, X_n)$. A statistic $T(X)$ is \textbf{sufficient} for
the model $\{P_{\theta}, \theta \in \Theta \}$ if the conditional 
distribution of $X \mid T(X)$ does not depend on $\theta$.
\end{definition}

\begin{remark}
Consider the following 2 stage procedure. Assume $T(X)$ is a sufficient
statistic for the model $\{P_{\theta}, \theta \in \Theta \}$.
 
\begin{enumerate}[label = (\arabic*)]
\item Suppose we observed data from $X \sim P_{\theta}$, 
$\theta \in \Theta$. Now calculate $T(X)$, keep it and discard $X$.

\item Generate $X^{\prime}$ from conditional distribution $X \mid T(X)$.
\end{enumerate}

For any $\theta \in \Theta$ calculate marginal distribution of new
$X^{\prime}$. Then

\begin{align*}
P_{\theta}(X^{\prime} = x) &= \sum_{t} P_{\theta}(X^{\prime} = x \mid T(X) = t)P_{\theta}(T(X) = t) \\
&= \sum_{t} P_{\theta}(X = x \mid T(X) = t)P_{\theta}(T(X) = t) \\
&= P_{\theta}(X = x)
\end{align*}

for any $X$.
\end{remark}

\begin{example}
Let $X = (X_1, X_2, \ldots, X_n) \overset{iid}{\sim} \text{Bernoulli}(\theta)$ where
$\theta \in \Theta = (0, 1)$. Let 
$T(X) = \sum_{i = 1}^n X_i \overset{iid}{\sim} \text{Binomial}(n, \theta)$. \\

Then

\begin{equation*}
P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n \mid T(X) = t) =
\begin{cases}
0 & \text{if } t \neq \sum_{i = 1}^n x_i \\
* & \text{if } t = \sum_{i = 1}^n x_i
\end{cases}
\end{equation*}

where 

\begin{equation*}
* = \frac{\theta^t (1 - \theta)^{n - t}}{\binom{n}{t} \theta^t (1 - \theta)^{n - t}} =
\frac{1}{\binom{n}{t}}
\end{equation*}

which does not depend on $\theta$.

Thus the $X \mid T(X)$ has a discrete uniform distribution,

\begin{equation*}
(X_1, \ldots, X_n) \mid T(X) = t \sim \text{uniform} \left\{ x_1, \ldots, x_n :
x_i \in \{0, 1\} \text{ and } \sum_{i = 1}^n x_i = t \right\}
\end{equation*}
\end{example}

\begin{remark}
In the above example $\sum_{i = 1}^{n-1} X_i$ is not a sufficient statistic. To
see this note that

$$ \mathbb{E}\left(X \; \middle|\;  \sum_{i =1}^{n-1} X_i = t \right) = \theta$$

which implies that the conditional distribution depends on $\theta$.
\end{remark}

\section{Maximum Likelihood}

\begin{example}
Let $X_1, \ldots, X_n \overset{iid}{\sim} uniform[0,\theta]$. Find the MLE for $\theta$.
\end{example}




\end{document}

