\documentclass[12pt]{article}

%\documentclass{amsart}
%\documentclass{scrartcl}
%\usepackage{changepage}
%\usepackage{scrextend}

\usepackage{amssymb,amsmath,amsthm}
% amssymb has empty set symbo
\usepackage{scrextend} % for \begin{addmargin}[0.55cm]{0cm} text \end{margin}

\usepackage{mathrsfs} % for \mathscr{P}
\usepackage{float}
\usepackage{enumitem}
\usepackage{hanging}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{pst-node}%

\newcommand{\n}{ \noindent }
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\U}{\mathcal{U}}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\newcommand{\pwset}{\mathcal{P}}
%\newcommand{\pwset}{\mathscr{P}}
\DeclareMathOperator{\LO}{\mathcal{L}}

\DeclareMathOperator{\proj}{proj}
\newcommand{\vct}{\mathbf}
\newcommand{\vctproj}[2][]{\proj_{\vct{#1}}\vct{#2}}


%https://tex.stackexchange.com/questions/22252/how-to-typeset-function-restrictions
\newcommand\restr[2]{{% we make the whole thing an ordinary symbol
  \left.\kern-\nulldelimiterspace % automatically resize the bar with \right
  #1 % the function
  \vphantom{\big|} % pretend it's a little taller at normal size
  \right|_{#2} % this is the delimiter
  }}

% from https://tex.stackexchange.com/questions/644238/drawing-the-phase-portrait-of-two-differential-equations
% \usepackage{pgfplots}
% \pgfplotsset{compat=1.8}
% \usepackage{amsmath}
% \usepackage{derivative}
\usepackage{pgfplots}
\pgfplotsset{compat=1.8}
\usetikzlibrary{decorations.markings}
\usepackage{amsmath}
\usepackage{derivative}


%\DeclareFontFamily{U}{MnSymbolC}{}
%\DeclareSymbolFont{MnSyC}{U}{MnSymbolC}{m}{n}
%\DeclareFontShape{U}{MnSymbolC}{m}{n}{
%  <-6>    MnSymbolC5
%  <6-7>   MnSymbolC6
%  <7-8>   MnSymbolC7
%  <8-9>   MnSymbolC8
%  <9-10>  MnSymbolC9
%  <10-12> MnSymbolC10
%  <12->   MnSymbolC12%
%}{}
%\DeclareMathSymbol{\powerset}{\mathord}{MnSyC}{180}

\newcommand{\overbar}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}
\newcommand{\overtilde}[1]{\mkern 1.5mu\widetilde{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[left=1.2in, right=1.2in, top=1in, bottom=1in]{geometry}

\newtheorem*{proposition}{Proposition}
\newtheorem*{lemma}{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem{conj}{Conjecture}[section]
\newtheorem*{example}{Example}
\newtheorem{theorem}{Theorem}[section]  % This enables \begin{theorem}
\newtheorem*{corollary}{Corollary}
\newtheorem*{claim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem*{result}{Result}

\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\DeclareMathOperator{\Tr}{Tr} 

\makeatletter
\newcommand{\@giventhatstar}[2]{\left(#1\;\middle|\;#2\right)}
\newcommand{\@giventhatnostar}[3][]{#1(#2\;#1|\;#3#1)}
\newcommand{\giventhat}{\@ifstar\@giventhatstar\@giventhatnostar}
\makeatother


\setcounter{MaxMatrixCols}{20}

\title{STAT 653 - Notes \\
Introduction to Mathematical Statistics}

\setcounter{tocdepth}{1}
%-1 part
%0 chapter     
%1 section       
%2 subsection  
%3 subsubsection
%4 paragraph
%5 subparagraph

\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}

\begin{document}
\maketitle
\tableofcontents

\section{Statistical Model}

\begin{example}
A coin is tossed $n$ times. The data available is $X = (X_1, X_2, \ldots, X_n)$, 
where $X_i \in \{0, 1\}$. The assumptions are:

\begin{enumerate}
\item outcomes are independent.
\item $P(X_i = 1) = \theta \in \Theta$ where $\theta$ is an unknown parameter
and $\Theta$ is the parameter space. In this case $\Theta = [0,1]$.
\end{enumerate}

We need to estimate $\theta$ based on the data $X = (X_1, X_2, \ldots, X_n)$,
where $X_i$ are random variables before the experiment is conducted. \\

So we need to find an estimator $T(X_1, X_2, \ldots, X_n)$ of $\theta \in \Theta$.\\

\underline{\textbf{Possible Estimators}}

\begin{enumerate}

\item $\displaystyle T_1 := T_1 (X_1, X_2, \ldots, X_n) = \overbar{X}_n = \frac{1}{n} \sum_{i = 1}^{n} X_i$

\begin{remark}
\begin{enumerate}[label = (\alph*)]
\item $\displaystyle \mathbb{E}(T_1) = \mathbb{E}(\overbar{X}_n) = \mathbb{E}(X_1) = \theta$
for all $\theta \in \Theta$ then $T_1$ is unbiased estimator of $\theta$.

\item $\lim_{n \rightarrow \infty} P(| \overbar{X}_n - \theta| > \epsilon ) = 0$ for all
$\epsilon > 0$.
\end{enumerate}
\end{remark}


\begin{definition}
In general, if $\displaystyle \lim_{n \rightarrow \infty} P(|T(X_1, \ldots, X_n) - \theta|  \epsilon) = 0$
for all $\epsilon > 0$ and for all $\theta \in \Theta$, then we call 
$T(X_1, \ldots, X_n)$ \textbf{consistent}.
\end{definition}

\item $T_2(X_1, \ldots, X_n) := X_1$, where $X_1 \in \{0, 1\}$. Then
$\mathbb{E}(T_2) = \mathbb{E}(X_1) = \theta$ for all $\theta \in \Theta$.\\

$T_2$ is unbiased but is not \underline{consistent}.

\item 
\begin{align*}
T_3 &:= T_3(X_1, \ldots, X_n) \\
&= \sqrt{\frac{1}{\lfloor \frac{n}{2} \rfloor} \sum_{i = 1}^{\lfloor \frac{n}{2} \rfloor} X_{2i} X_{2i-1} }
\end{align*}

$T_3$ is biased because

\begin{align*}
\mathbb{E}(T_3) &\leq \sqrt{\frac{1}{\lfloor \frac{n}{2} \rfloor} \sum_{i = 1}^{\lfloor \frac{n}{2} \rfloor} X_{2i} X_{2i-1} } \\
&= \theta \quad\quad \forall \theta \in \Theta
\end{align*}

\end{enumerate}
\end{example}

\begin{example}
Suppose $X_1, X_2, \ldots, X_n$ are independent and have uniform$[0, \theta]$,
where $theta \in \Theta = \mathbb{R}_+$. So $\Theta = \{\theta : \theta > 0\}$. \\


\underline{\textbf{Possible Estimators}}

\begin{enumerate}
\item $T_1(X_1, \ldots, X_n) = 2\overbar{X}_n$

\item $T_2(X_1, \ldots, X_n) = X_{(n)}$ (max)

\item $T_3(X_1, \ldots, X_n) = c_n X_{(n)}$

Correct the max by a constant so it is unbiased.
\end{enumerate}
\end{example}

\begin{example}
We want to receive a shipment of oranges and suspect that part of them
rot off. To check the shipment we draw a random sample without replacement
of size $n$ from the shipment (population) of size $N$.\\

Let $\theta$ be the proportion of bad oranges in the population. So
$\Theta = \{ \frac{0}{N}, \frac{1}{N}, \ldots, \frac{N}{N} \}$.\\

Let

\begin{equation*}
X_i =
\begin{cases*}
0 & if good \\
1 & if bad
\end{cases*}
\end{equation*}

for $i = 1, 2, \ldots, n$ and let $X = (X_1, X_2, \ldots, X_n)$.\\

Let $\displaystyle T_1(X) = \sum^{n}_{i = 1} X_i$. Then $T_1$ has a
hypergeometric distribution. So

\begin{equation*}
P_{\theta}(X_1 = k) = \frac{ \left( \frac{N\theta}{k} \right) \left( \frac{N - N\theta}{n-k} \right)}
{ \left( \frac{N}{n} \right) }
\end{equation*}

for $k \in \{ \max(0, n - (N - N\theta), \ldots, \min(n, N\theta))\}$
\end{example}

\section{The Likelihood Function}

$$ X \sim P_{\theta}, \quad\quad \theta \in \Theta $$

We have 2 cases for now (discrete and continuous):

\begin{enumerate}[label = (R\arabic*)]
\item $P_{\theta}$ is defined by a joint pdf $f_X (x; \theta)$ for all $\theta \in \Theta$.
\item $P_{\theta}$ is defined by a joint pmf $P(X = x; \theta)$ for all $\theta \in \Theta$.
\end{enumerate}

\begin{definition}
Let $P_{\theta}$, $\theta \in \Theta$ be a model satisfying (R1) or (R2). Then the function

\begin{equation*}
L(x; \theta) =
\begin{cases*}
f_X(x; \theta) & if (R1) \\
P(X = x; \theta) & if (R2)
\end{cases*}.
\end{equation*}
\end{definition}

\begin{example}
Not (R1) and not (R2).

Let

$$ X \sim N(\theta, 1) \quad\quad \theta \in \Theta = \mathbb{R}. $$

We observe $Y = \max(0, X)$, 

\begin{equation*}
Y =
\begin{cases*}
0 & if $X \leq 0 $\\
X & if $ X > 0$
\end{cases*}
= X I(X > 0)
\end{equation*}

where $I(\cdot)$ is the indicator function.\\

$F_{\theta} (t) = P(Y \leq t)$ for all $t \in \mathbb{R}$.
\end{example}

\begin{example}
Back to oranges example where $X = (X_1, X_2, \ldots, X_n)$ is the data
and $\Theta = \{ \frac{0}{N}, \frac{1}{N}, \ldots, \frac{N}{N} \}$. Let
$\displaystyle T(X) = \sum^{n}_{i=1} X_i$. Then

\begin{align*}
L(x; \theta) &= P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n) \\
&= P_{\theta}\left(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n, T(X) = \sum^{n}_{i=1} x_i \right) \\
&= P_{\theta}\left(T(X) = \sum^{n}_{i=1} x_i\right)P\left(X_1 = x_1, \ldots, X_n = x_n \; \middle|\;  T(X) = \sum^{n}_{i=1} x_i \right).
\end{align*}


Now define $K_n = \sum^{n}_{i=1} x_i$. For example, if $n = 5$ and 
we observed $(1, 0, 0, 1, 1)$ then

$$ K = \sum_{i=1}^{5} x_i = 3. $$

Since there are 10 possibilities for which entries are 1 versus 0,
$\binom{5}{3} = 10$. Because all possible combinations of 1 and 0 
are possible we can use symmetry to calculate the probability of
any particular sequence of 1 and 0 as $1/\binom{5}{3}$. We use 
this reasoning below to derive the expression on the right. 

Then

\begin{equation*}
L(x; \theta) = \frac{ \binom{N\theta}{K_n} \binom{N-N\theta}{n - K_n}}
{\binom{N}{n}} \times \frac{1}{\binom{n}{K_n}}.
\end{equation*}
\end{example}

\section{Identifiability of Statistical Models}

\begin{definition}
Let $X \sim P_{\theta}$, $\theta \in \Theta$. A model $P_{\theta}$,
$\theta \in \Theta$ is \underline{identifiable} if for any pair 
$(\theta, \theta^{\prime})$ such that $\theta \neq \theta^{\prime}$
and $\theta, \theta^{\prime} \in \Theta$, then
$P_{\theta} \neq P_{\theta^{\prime}}$.
\end{definition}

\begin{remark}
This means that there is an event $A$, such that 
$P_{\theta}(A) \neq P_{\theta^{\prime}}$ where 
$\theta \neq \theta^{\prime}$.

\begin{enumerate}[label = R(\arabic*)]
\item For $\theta \neq \theta^{\prime}$, 
$f(x; \theta) \neq f(x; \theta^{\prime})$ for any neighborhood of
$x$ (an open ball $B(x, r)$ centered at $x$).

By open ball we mean $B(x, r) = \{y : |x-y| < \epsilon \}$ where 
$|v| = (\sum^{n}_{i = 1} v_i^2 )^{1/2}$ (euclidean norm).

\item Discrete support, for some $x$
$P_{\theta}(X = x) \neq P_{\theta^{\prime}}(X = x)$ where
$\theta \neq \theta^{\prime}$.

\end{enumerate}
\end{remark}

\begin{example}
Suppose we observe $X_1, X_2, \ldots, X_n$ where 
$X_i = \theta \cdot Z_i \sim N(0, \theta^2)$ and
$Z_i \sim N(0,1)$ and $\theta \in \Theta = \mathbb{R} \setminus \{0\}$.\\

If $\theta_1 = 1 \neq -1 = \theta_2$, then

\begin{equation*}
L(x_1, x_2, \ldots, x_n; \theta = 1) = L(x_1, x_2, \dots, x_n; \theta = -1)
\end{equation*}

for any $x = (x_1, \ldots, x_n)$.
\end{example}

\begin{result}
The model $\{P_{\theta}, \theta \in \Theta \}$ is identifiable if there exists 
a statistic $T(X)$ ($X \sim P_{\theta}, \theta \in \Theta$) where 
expectation is a one-to-one function of $\theta \in \Theta$, i.e., such that

\begin{equation}\label{eq1}
\forall (\theta, \theta^{\prime}), \quad \theta \neq \theta^{\prime} \implies 
\mathbb{E}_{\theta}(T(X)) \neq \mathbb{E}_{\theta^{\prime}}(T(X))
\end{equation}

\begin{proof}
We use proof by contradiction. Suppose that (\ref{eq1}) holds, but there
exists $\theta \neq \theta^{\prime}$ such that $P_{\theta} = P_{\theta^{\prime}}$.
If so, then $\mathbb{E}_{\theta}(T(X)) = \mathbb{E}_{\theta^{\prime}}(T(X))$,
which contradicts (\ref{eq1}).
\end{proof}
\end{result}


In the previous example, $\theta = 1$, $\theta^{\prime} = -1$.

\begin{example}
Let $X_1, X_2, \ldots, X_n \overset{\mathrm{iid}}{\sim} \text{Bernoulli}(\theta)$
where $\theta \in \Theta = [0, 1]$. We will show that $\theta$ is identifiable
using the definition and also the above result.\\

Let $\theta$ and $\theta^{\prime}$ be arbitrary and suppose 
$\theta \neq \theta^{\prime}$ and $\theta^{\prime}$,$\theta \in \Theta$. Also
suppose $X = (1, 1, \ldots, 1)$. Then

\begin{align*}
P_{\theta}(X_1, X_2, \ldots, X_n) &= \theta^{n}\\
P_{\theta^{\prime}}(X_1 = 1, \ldots, X_n) &= (\theta^{\prime})^{n}.
\end{align*}

Since $\theta \in [0,1]$ then $\theta^n \neq (\theta^{\prime})^n$ and the model
is identifiable.\\

Now take a statistic $T(X_1, \ldots, X_n) = X_1$ (or we could take 
$T(X_1, \ldots, X_n) = \sum^{n}_{i=1} X_i$ or
$T(X_1, \ldots, X_n) = \sum^{n}_{i=1} \overbar{X}_n$).\\

For any 
$(\theta, \theta^{\prime}) \in \Theta$, if $\theta \neq \theta^{\prime}$
then $\mathbb{E}_{\theta}(\overbar{X}_n) = \theta \neq \theta^{\prime} = 
\mathbb{E}_{\theta^{\prime}}(\overbar{X}_n)$. Then by the above result
the model is identifiable.
\end{example}

\begin{example}
\begin{equation*}
X_1, \ldots, X_n \overset{iid}{\sim} N(\mu, \sigma^2)
\end{equation*}

\begin{enumerate}[label = Part \arabic*)]

\item 
Let $\theta = (\mu, \sigma^2) \in \Theta = \mathbb{R} \times \mathbb{R}^2$. Then

\begin{equation*}
L(x_1, \ldots, x_n; \theta) = \frac{1}{\sqrt{2 \pi \sigma^2}} 
e^{\frac{-(x_i - \mu)^2}{2\sigma^2}} I(\mu \in \mathbb{R})I(\sigma^2 > 0).
\end{equation*}

It is difficult in this case to use the definition to show identifiability
in this case, but we can use the previous result.

We are given $X = (X_1, X_2, \dots, X_N)$. Let 

\begin{equation*}
T(X) = \left( \sum_{i = 1}^n X_i,  \sum^n_{i = 1} X_i^2 \right)
\end{equation*}

Then 
\begin{align*}
\mathbb{E}_{\theta}(T) &= (n \mu, n(\sigma^2 + \mu^2)),\\
\mathbb{E}_{\theta^{\prime}}(T) &= (n \mu^{\prime}, n(\sigma^{2^{\prime}} + (\mu^{\prime})^2) 
\end{align*}

Thus, if $(\theta, \theta^2) \in \Theta$ then
\begin{equation*}
\forall \theta \neq \theta^{\prime} \implies \mathbb{E}_{\theta}(T(X)) \neq \mathbb{E}_{\theta^{\prime}}(T(X)).
\end{equation*}

If $\theta \neq \theta^{\prime}$ then $\mu \neq \mu^{\prime}$ or 
$\sigma^2 \neq \sigma^{2^{\prime}}$ or $\mu \neq \mu^{\prime}$ and
$\sigma^2 \neq \sigma^{2^{\prime}}$. In all three cases then
$\mathbb{E}_{\prime}(T(X)) \neq \mathbb{E}_{\theta^{\prime}}(T(X))$.

\item Suppose we observe only $Y_1, \ldots, Y_n$ where

\begin{equation*}
Y_i =
\begin{cases}
+1 & \text{if } X_i \geq 0 \\
-1 & \text{if } X_i < 0.
\end{cases}
\end{equation*}

Since $Y_i = g(X_i)$ and the $X_i$'s are independent, then the $Y_i$'s
are also independent.

Then the likelihood function is

\begin{align*}
L(y_i, \ldots, y_n; \theta) &= \prod_{i = 1}^n P(Y_i = y_i; \theta) \\
&= \prod_{i = 1}^n \left[ I(y_i = 1) P(X_i \geq 0) + I(y_i = -1)P(X_i < 0) \right].
\end{align*}

Now note that

\begin{align*}
P(X_i \geq 0) &= 1 - P(X_i < 0) = 1 - \Phi \left( -\frac{\mu}{\sigma} \right) = \Phi \left( \frac{\mu}{\sigma} \right)\\
P(X_i < 0) &= \Phi \left( -\frac{\mu}{\sigma} \right)
\end{align*}

so that only the ratio $\mu/\sigma$ matters for the the likelihood.\\

Now let $\theta = (3, 9) \neq (4, 16) = \theta^{\prime}$. For $\theta$ we
have $\mu/\sigma = 3/3 = 1$ and for $\theta^{\prime}$ we have
$\mu/\sigma = 4/4 = 1$. Thus we have

$$ \theta = (3,9) \neq (4,16) = \theta^{\prime} \implies L(y; \theta) = L(y; \theta^{\prime}) $$

and so the model is not identifiable. For any $y = (y_1, \ldots, y_n)$ we
have $L(y; \theta) = L(y; \theta^{\prime})$ and thus the model is
not identifiable.


\begin{remark}
Above we used the fact that for a general normal random variable $N(\mu, \sigma^2)$,
$F(x) = \Phi((x - \mu)/\sigma)$.
\end{remark}
\end{enumerate}
\end{example}

\section{Sufficient Statistic}

\begin{definition}
Let $X \sim P_{\theta}$, $\theta \in \Theta$ and we observe data 
$X = (X_1, \ldots, X_n)$. A statistic $T(X)$ is \textbf{sufficient} for
the model $\{P_{\theta}, \theta \in \Theta \}$ if the conditional 
distribution of $X \mid T(X)$ does not depend on $\theta$.
\end{definition}

\begin{remark}
Consider the following 2 stage procedure. Assume $T(X)$ is a sufficient
statistic for the model $\{P_{\theta}, \theta \in \Theta \}$.
 
\begin{enumerate}[label = (\arabic*)]
\item Suppose we observed data from $X \sim P_{\theta}$, 
$\theta \in \Theta$. Now calculate $T(X)$, keep it and discard $X$.

\item Generate $X^{\prime}$ from conditional distribution $X \mid T(X)$.
\end{enumerate}

For any $\theta \in \Theta$ calculate marginal distribution of new
$X^{\prime}$. Then

\begin{align*}
P_{\theta}(X^{\prime} = x) &= \sum_{t} P_{\theta}(X^{\prime} = x \mid T(X) = t)P_{\theta}(T(X) = t) \\
&= \sum_{t} P_{\theta}(X = x \mid T(X) = t)P_{\theta}(T(X) = t) \\
&= P_{\theta}(X = x)
\end{align*}

for any $X$.
\end{remark}

\begin{example}
Let $X = (X_1, X_2, \ldots, X_n) \overset{iid}{\sim} \text{Bernoulli}(\theta)$ where
$\theta \in \Theta = (0, 1)$. Let 
$T(X) = \sum_{i = 1}^n X_i \overset{iid}{\sim} \text{Binomial}(n, \theta)$. \\

Then

\begin{equation*}
P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n \mid T(X) = t) =
\begin{cases}
0 & \text{if } t \neq \sum_{i = 1}^n x_i \\
* & \text{if } t = \sum_{i = 1}^n x_i
\end{cases}
\end{equation*}

where 

\begin{equation*}
* = \frac{\theta^t (1 - \theta)^{n - t}}{\binom{n}{t} \theta^t (1 - \theta)^{n - t}} =
\frac{1}{\binom{n}{t}}
\end{equation*}

which does not depend on $\theta$.

Thus the $X \mid T(X)$ has a discrete uniform distribution,

\begin{equation*}
(X_1, \ldots, X_n) \mid T(X) = t \sim \text{uniform} \left\{ x_1, \ldots, x_n :
x_i \in \{0, 1\} \text{ and } \sum_{i = 1}^n x_i = t \right\}
\end{equation*}
\end{example}

\begin{remark}
In the above example $\sum_{i = 1}^{n-1} X_i$ is not a sufficient statistic. To
see this note that

$$ \mathbb{E}\left(X \; \middle|\;  \sum_{i =1}^{n-1} X_i = t \right) = \theta$$

which implies that the conditional distribution depends on $\theta$.
\end{remark}

\begin{example}
$$X_1, X_2, \ldots, X_n \overset{\text{iid}}{\sim} N(\theta, 1) \quad\quad \theta \in \Theta = \mathbb{R}$$
\end{example}

Let $\displaystyle T(X) = \sum_{i = 1}^n X_i = \overbar{X}_n$. Then

\begin{align*}
\left.\
\begin{bmatrix}
X_1 \\
X_2 \\
\vdots \\
X_n
\end{bmatrix} 
\;\middle|\;
\overbar{X}_n = t
\right.\
\sim
N \left(
\begin{bmatrix}
t \\
t \\
\vdots \\
t
\end{bmatrix},
\begin{bmatrix}
1 - \frac{1}{n}, & -\frac{1}{n}, & \ldots, & -\frac{1}{n} \\
-\frac{1}{n}, & 1 - \frac{1}{n}, & \ldots, & -\frac{1}{n} \\
\vdots & & & \\
-\frac{1}{n}, & \ldots, & -\frac{1}{n}, & 1 - \frac{1}{n}
\end{bmatrix}
\right)
\end{align*}

where the multivariate normal distribution on the right does not
depend on $\theta$. Thus $\overbar{X}_n$ is sufficient for this
model.



\section{Fisher-Neyman Factorization Theorem}

Consider the model $X \sim P_{\theta}$, $\theta \in \Theta$. Then $T(X)$
is sufficient statistic for $P_{\theta}$ if and only if there exists
functions $g(\theta, t)$ and $h(x)$ (with appropriate domains) such that

\begin{equation*}
L(x; \theta) = g(\theta, T(x))h(x) \quad\quad \forall x; \forall \theta \in \Theta
\end{equation*} 

\begin{proof}
\begin{enumerate}[label = (\Roman*)]
\item Sufficient condition\\

Assume  holds and we must show that $T(X)$ is sufficient. We do only
the discrete case.

\begin{equation*}
P_{\theta}(X = x \mid T(X) = t) =
\begin{cases}
0 & \text{if } T(x) \neq t \\
* & \text{if } T(x) = t
\end{cases}  
\end{equation*}

where $*$ is

\begin{align*}
* &= \frac{P_{\theta}(X = x)}{P_{\theta}(T(x) = t)} 
= \frac{P_{\theta}(X = x)}{\sum\limits_{y:T(y) = t} P_{\theta}(X = y)} \\
&= \frac{g(\theta, T(x) = t)h(x)}{\sum\limits_{y:T(y) = t} g(\theta, T(x) = t)h(y)} \\
&= \frac{h(x)}{\sum\limits_{y:T(y) = t} h(y)}.
\end{align*}

Since the final expression above does not depend on $\theta$, which implies
that $T(X)$ is a sufficient statistic for the given model.

\item Necessary Condition \\

Now assume that $T(X)$ is a sufficient statistic for the given model. Then

\begin{align*}
P_{\theta}(X = x) &= P_{\theta}(X = x, T(x) = T(x)) \\
&= P(X = x \mid T(x) = t_x)P_{\theta}(T(x) = t_x) \\
&= h(x)g(\theta, t_x).
\end{align*}

\end{enumerate}
\end{proof}


\begin{example}
Let $X_1, X_2, \ldots, X_n \overset{\text{iid}}{\sim} \text{Bernoulli}(\theta)$
where $X_i \in \{0, 1\}$. Then the likelihood is

\begin{align*}
L(x; \theta) &= P_{\theta}(X_1 = x_1, \ldots, X_n = x_n) \\
&= \theta^{\sum\limits_{i = 1}^n x_i} (1 - \theta)^{n - \sum\limits_{i = 1}^n x_i}
\prod_{i=1}^n I(x_i \in \{0, 1\}) \\
&= g\left(\theta, T(x) = \sum_{i = 1}^n  x_i \right) h(x).
\end{align*}
 
Thus, $T(X) = \sum_{i = 1}^n X_i$ is sufficient for this model.
\end{example}

\begin{example}
Let $X_1, X_2, \ldots, X_n \overset{\text{iid}}{\sim} N(\theta, 1)$. Then the 
likelihood is

\begin{align*}
L(x; \theta) &= \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi}} e^{-\frac{(x_i - \theta)^2}{2}}
\prod_{i = 1}^n I(-\infty < x_i < \infty) \\
&= e^{\theta (\sum\limits_{i = 1}^n x_i) - \frac{n \theta^2}{2}} \left( \frac{1}{\sqrt{2\pi}} \right)^n
e^{-\frac{1}{2} \sum\limits_{i = 1}^n x_i^2} \prod_{i = 1}^n I(-\infty < x_i < \infty) \\
&= \left[ e^{\theta n \overbar{X}_n - \frac{n \theta^2}{2}} \right] \left[ \left( \frac{1}{\sqrt{2\pi}} \right)^n
e^{-\frac{1}{2} \sum\limits_{i = 1}^n x_i^2} \prod_{i = 1}^n I(-\infty < x_i < \infty) \right] \\
&= g(\theta, \overbar{X}_n)h(x)
\end{align*}

Thus, $\overbar{X}_n$ is a sufficient statistic for this model.

\end{example}


\begin{example}

\begin{equation*}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \sim
N \left(
\begin{bmatrix}
0 \\
0
\end{bmatrix}
,
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
\right),
\quad
-1 < \rho < 1,
\quad
-\infty < x_1, x_2, < \infty
\end{equation*}

Suppose we have on observation $x = (x_1, x_2)$. Then 
the likelihood is

\begin{align*}
L(x ; \rho) &= \frac{1}{2 \pi \sqrt{1 - \rho}} e^{-\frac{x_1^2 + x_2^2 - 2\rho x_1 x_2}{2(1 - \rho^2)}}
I(-\infty < x_1, x_2 < \infty) \\
&=g(\rho; T(x) = (x_1^2 + x^2_2, x_1 x_2)) h(x)
\end{align*}

where $h(x) = 1$. Now suppose we have $n$ observations

\begin{equation*}
x =
\left(
\begin{bmatrix}
x_{11} \\
x_{21}
\end{bmatrix},
\begin{bmatrix}
x_{12} \\
x_{22}
\end{bmatrix},
\ldots,
\begin{bmatrix}
x_{1n} \\
x_{2n}
\end{bmatrix}
\right)
\end{equation*}

where each vector in $x$ is independent of all others. Then the sufficient
statistic $T(X)$ is

\begin{equation*}
T(x) = \left( \sum_{j=1}^n (x_{1j}^2 + x_{2j}^2 ), \sum_{j = 1}^n x_{1j}x_{2j} \right).
\end{equation*}
\end{example}

\section{Exchangable Random Variables}

\begin{definition}
The random variables $X_1, X_2, \ldots, X_n$ are \textbf{exchangable} 
random variables if

\begin{equation*}
(X_1, \ldots, X_n) \sim (X_{\pi(1)}, \ldots, X_{\pi(n)})
\end{equation*}

for any permutation $\pi(1), \ldots, \pi(n)$ of integers $1, 2, \ldots, n$.
\end{definition}

\begin{remark}
If $X_1, \ldots, X_n$ are identically and independently distributed 
$\implies$ $X_1, \ldots, X_n$ are exchangable. However, $X_1, \ldots, X_n$
are exchangable $\notimplies$ $X_1, \ldots, X_n$ are identically and
independently distributed.
\end{remark}

\begin{example}
\begin{equation*}
P(X_1 = x_1, X_2 = x_2) = P(X_2 = x_2, X_1 = x_1)
\end{equation*}
\end{example}

\begin{example}

\begin{equation*}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \sim
N \left(
\begin{bmatrix}
0 \\
0
\end{bmatrix}
,
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
\right),
\quad
-1 < \rho < 1,
\quad
-\infty < x_1, x_2, < \infty
\end{equation*}

Suppose we have on observation $x = (x_1, x_2)$.

Then $f_{x_1, x_2}(x_1, x_2 ; \rho) = f_{x_1, x_2}(x_2, x_1; \rho)$ so that
$(X_1, X_2) \sim (X_2, X_1)$.
\end{example}

\begin{result}
If $(X_1, \ldots, X_n) \sim P_{\theta}$, $\theta \in \Theta$ are
exchangable random variables then 

$$ T(X) = (X_{(1)}, \ldots, X_{(n)}) $$

is a sufficient statistic for $P_{\theta}$, $\theta \in \Theta$ where
$T(X)$ is the vector of order statistics.
\end{result}

\begin{proof}
Let $(X_1, \ldots, X_n) \sim P_{\theta}$, $\theta \in \Theta$ are
exchangable random variables. Let $y_1 \leq y_2 \leq \ldots \leq y_n$ 
be the observed order statistics. Then

\begin{align*}
P_{\theta}(X_1 = x_1, X_2, = x_2, \ldots, X_n = x_n \mid X_{(1)} = y_1, \ldots, X_{(n)} = y_n) \\
= \begin{cases}
* & \text{if } \{x_1, \ldots x_n \} = \{y_1, \ldots, y_n \} \\
0 & \text{if } \{x_1, \ldots, x_n \} \neq \{y_1, \ldots, y_n \}
\end{cases}
\end{align*}

where

\begin{align*}
* &= \frac{P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)}
{P_{\theta}(X_{(1)} = y_1, \ldots, X_{(n)} = y_n)} \\
&= \frac{P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)}
{\sum\limits_{\substack{ \text{all} \\ \text{possible} \\ \text{permutations}}} 
P_{\theta}(X_1 = y_{\pi(1)}, \ldots, X_n = y_{\pi (n)})} \\
&= \frac{P_{\theta}(X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n)}
{\sum\limits_{\substack{ \text{all} \\ \text{possible} \\ \text{permutations}}} 
P_{\theta}(X_1 = x_{\pi(1)}, \ldots, X_n = x_{\pi (n)})} \\
&= \frac{P_{\theta}(X_1 = x_1, \ldots, X_n = x_n)}
{n! P_{\theta}(X_1 = x_1, \ldots, X_n = x_n)} \\
&= \frac{1}{n!}.
\end{align*}


Note that $1/n!$ does not depend on $\theta$ for all $\theta \in \Theta$. Therefore,
by definition, the vector of order statistics is sufficient for model
$P_{\theta}$, $\theta \in \Theta$.
\end{proof}

\begin{example}
\begin{equation*}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \sim
N \left(
\begin{bmatrix}
0 \\
0
\end{bmatrix}
,
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
\right),
\quad
-1 < \rho < 1,
\quad
-\infty < x_1, x_2, < \infty
\end{equation*}

Suppose we have on observation $x = (x_1, x_2)$.

From the previous result we have $T(x) = (x_{(1)}, x_{(2)})$ is
sufficient. \\

We know already that $T_1(x) = (x_1^2 + x_2^2, x_1 x_2)$ is sufficient and
we just showed that $T_2(x) = (x_{(1)}, x_{(2)})$ is also sufficient. \\

Then $T_1$ is a function of $T_2$. So if we know $T_2$ then we can calculate
$T_1$, but not vice versa. This leads to the next definition.
\end{example}

\section{Minimal Sufficient Statistic}

\begin{definition}
Let $X \sim P_{\theta}$, $\theta \in \Theta$. The sufficient statistic, $S(X)$, 
is minimal sufficient if there is a function of any other sufficient statistic.
This means that for any sufficient statistic $T(X)$ there exists a function
$f:S(X) = f(T(X))$.
\end{definition}

We can use the following lemma to check for minimal sufficiency.

\begin{lemma}
Let $X \sim P_{\theta}$, $\theta \in \Theta$. A sufficient statistic $S(X)$
is minimal sufficient if

\begin{equation*}
\frac{L(x; \theta)}{L(y; \theta)} \text{ does not depend on } \theta \implies
S(x) = S(y), \quad \forall \theta \in \Theta.
\end{equation*}
\end{lemma}

\begin{proof}
Let $x, y$ be such that $T(x) = T(y)$. Suppose the implication in the lemma
holds and let $T(X)$ be a sufficient statistic. Then

\begin{equation*}
\frac{L(x; \theta)}{L(y; \theta)} = \frac{g(\theta, T(x))h(x)}{g(\theta, T(y))h(y)}
= \frac{h(x)}{h(y)}
\end{equation*}

which does not depend on $theta$, for all $\theta \in \Theta$. Then this implies
that $S(x) = S(y)$. \\

Recall that $T(x)$ is arbitrary. Above we showed that $T(x) = T(y) \implies S(x) = S(y)$.
Now choose another sufficient statistic, $T_1(X)$. Then 
$T_1(x) = T_1(y) \implies S(x) = S(y)$. Then if follows that

$$ S(x) = f(T(x)).$$

We won't write the formal proof because it will take a while.
\end{proof}

\begin{remark}


\begin{enumerate}[label = (\arabic*)]
\item If $S(x) = S(y)$ then 

\begin{equation*}
\frac{g(\theta; S(x))h(x)}{g(\theta, S(y))h(y)} = \frac{h(x)}{h(y)}
\end{equation*}

which does not depend on $\theta$ for all $\theta \in \Theta$.

\item $A \implies B$ is equivalent to $B^c \implies A^c$.

\item The meaning of the left hand side of the implication in the above
lemma is

\begin{equation*}
L(x; \theta) = c(x, y)L(y; \theta), \quad \forall \theta \in \Theta.
\end{equation*}

\end{enumerate}
\end{remark}

\begin{example}
Let $X = (X_1, \ldots, X_n)$ where $X_i \overset{iid}{\sim} N(\theta, 1)$ where
$\theta \in \Theta = \mathbb{R}$.
\end{example}


\section{Ancillary Statistic}

\begin{definition}
Let $X \sim P_{\theta}$, $\theta \in \Theta$.
\begin{enumerate}[label = (\arabic*)]

\item A statistic, $A(X)$, whose distribution does not depend on $\theta$
is called \textbf{ancillary}.

\item If $\mathbb{E}_{\theta}(A^{*}(X))$ does not depend on $\theta$ then
$A^{*}(X)$ is \textbf{first-order ancillary}.
\end{enumerate}
\end{definition}

\begin{remark}
Ancillary $\implies$ first-order ancillary, but first-order ancillary
$\notimplies$ ancillary.
\end{remark}

\begin{example}
Let $X_1, X_2 X_n \overset{iid}{\sim} N(\theta, 1), \quad \theta \in \Theta = \mathbb{R}$.\\

A sufficient statistic for this model is
 
$$ S(X) = X_1 + X_2 \sim N(2\theta, 2). $$

An ancillary statistic for this model is 

$$ A(X) = X_1 - X_2 \sim N(0, 2). $$
\end{example}

\begin{example}

\begin{equation*}
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\sim
N \left(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\begin{bmatrix}
1 & \rho \\
\rho & 1
\end{bmatrix}
\right),
\quad
-1 < \rho < 1
\end{equation*}

In this example $\theta = \rho$. A sufficient statistic for this model is

$$ S(X) = (X_1^2 + X_2^2, X_1 X_2). $$

Note $X_1 \sim N(0, 1)$ and $X_2 \sim N(0, 1)$ are not independent. \\

Then two ancillary statistics are

\begin{align*}
A_1(X) &= X_1 \\
A_2(X) &= X_2.
\end{align*}

A first-order ancillary statistic is

$$ A^* (X) = X_1^2 + X_2^2 $$

because $\mathbb{E}_{\theta} (A^* (X) ) = 2$.\\

Another ancillary statistic is 

$$ A(X) = I(-1 \leq X_1 < 1) + I(-1 \leq X_2 \leq 1). $$

\end{example}

\section{Scale and Location Family}

\begin{enumerate}

\item \underline{Location Family}

Let $X \sim F$ where $F$ is some distribution function that does not depend
on any unknown parameters, e.g., $N(0,1)$. Let $\theta \in \Theta = \mathbb{R}$ and
define

$$ Y = X + \theta. $$

Then $Y$ is in a location family. The distribution function for $Y$ is

$$ F_Y(t) = P(Y \leq t) = P(X \leq t - \theta) = F_X(t - \theta). $$

\end{enumerate}

\section{Maximum Likelihood}

\begin{example}
Let $X_1, \ldots, X_n \overset{iid}{\sim} uniform[0,\theta]$. Find the MLE for $\theta$.
\end{example}




\end{document}

